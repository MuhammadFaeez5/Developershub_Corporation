{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import gradio as gr\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Load and prepare the dataset\n",
        "def load_and_prepare_data():\n",
        "    # Load AG News dataset from Hugging Face\n",
        "    dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "    # Get class names\n",
        "    class_names = dataset['train'].features['label'].names\n",
        "\n",
        "    # Split the dataset\n",
        "    train_data = dataset['train']\n",
        "    test_data = dataset['test']\n",
        "\n",
        "    # For demonstration, we'll use a smaller subset\n",
        "    train_data = train_data.select(range(10000))\n",
        "    test_data = test_data.select(range(2000))\n",
        "\n",
        "    return train_data, test_data, class_names\n",
        "\n",
        "# Step 2: Tokenization\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def tokenize_data(train_data, test_data):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenize the text\n",
        "    train_texts = train_data['text']\n",
        "    test_texts = test_data['text']\n",
        "\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = NewsDataset(train_encodings, train_data['label'])\n",
        "    test_dataset = NewsDataset(test_encodings, test_data['label'])\n",
        "\n",
        "    return train_dataset, test_dataset, tokenizer\n",
        "\n",
        "# Step 3: Model Training\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "def train_model(train_dataset, test_dataset, class_names):\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=len(class_names)\n",
        "    ).to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 4: Evaluation\n",
        "def evaluate_model(model, test_dataset):\n",
        "    trainer = Trainer(model=model)\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    metrics = compute_metrics(predictions)\n",
        "    return metrics\n",
        "\n",
        "# Step 5: Deployment with Gradio\n",
        "def create_gradio_interface(model, tokenizer, class_names):\n",
        "    def classify_headline(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        top_prob, top_label = torch.max(probs, dim=1)\n",
        "\n",
        "        return {\n",
        "            \"Predicted Category\": class_names[top_label.item()],\n",
        "            \"Confidence\": f\"{top_prob.item():.2f}\"\n",
        "        }\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=classify_headline,\n",
        "        inputs=gr.Textbox(label=\"News Headline\", placeholder=\"Enter news headline here...\"),\n",
        "        outputs=gr.Label(label=\"Classification Result\"),\n",
        "        title=\"News Topic Classifier\",\n",
        "        description=\"Classify news headlines into categories using BERT\",\n",
        "        examples=[\n",
        "            [\"Apple announces new iPhone with revolutionary features\"],\n",
        "            [\"Football team wins championship after dramatic final match\"],\n",
        "            [\"Stock market reaches all-time high amid economic recovery\"],\n",
        "            [\"Scientists discover new species in Amazon rainforest\"]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Step 1: Load data\n",
        "    train_data, test_data, class_names = load_and_prepare_data()\n",
        "    print(f\"Class names: {class_names}\")\n",
        "    print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    train_dataset, test_dataset, tokenizer = tokenize_data(train_data, test_data)\n",
        "\n",
        "    # Step 3: Train model\n",
        "    model = train_model(train_dataset, test_dataset, class_names)\n",
        "\n",
        "    # Step 4: Evaluate\n",
        "    metrics = evaluate_model(model, test_dataset)\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "    # Step 5: Deploy\n",
        "    print(\"\\nLaunching Gradio interface...\")\n",
        "    interface = create_gradio_interface(model, tokenizer, class_names)\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7sLzjk1-wn0N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rEXPIst12rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1XBN5DpwqaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Du1XK_hQwqdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers  scikit-learn gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzJs5Dd_wqhO",
        "outputId": "6d5752ae-50ed-4c81-e1e5-ae088137062b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.37.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.4 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.4)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available and enable mixed precision\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mixed precision training - Removed explicit autocast\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # torch.cuda.amp.autocast(enabled=True) # Removed this line\n",
        "\n",
        "# Step 1: Load and prepare the dataset (optimized)\n",
        "def load_and_prepare_data():\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')} - Loading dataset...\")\n",
        "    dataset = load_dataset(\"ag_news\", streaming=False)  # Disable streaming for faster access\n",
        "\n",
        "    class_names = dataset['train'].features['label'].names\n",
        "\n",
        "    # Use smaller subsets for demonstration\n",
        "    train_data = dataset['train'].select(range(8000))  # Reduced from 10000\n",
        "    test_data = dataset['test'].select(range(1000))    # Reduced from 2000\n",
        "\n",
        "    # Convert to lists\n",
        "    train_texts = train_data['text']\n",
        "    train_labels = train_data['label']\n",
        "    test_texts = test_data['text']\n",
        "    test_labels = test_data['label']\n",
        "\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')} - Dataset loaded\")\n",
        "    return train_texts, train_labels, test_texts, test_labels, class_names\n",
        "\n",
        "# Optimized Dataset class\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
        "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def tokenize_data(train_texts, train_labels, test_texts, test_labels):\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')} - Tokenizing data...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenize in batches\n",
        "    train_encodings = tokenizer(\n",
        "        list(train_texts),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=96,  # Reduced from 128\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    test_encodings = tokenizer(\n",
        "        list(test_texts),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=96,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')} - Tokenization complete\")\n",
        "    return (\n",
        "        NewsDataset(train_encodings, list(train_labels)),\n",
        "        NewsDataset(test_encodings, list(test_labels)),\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n",
        "\n",
        "def train_model(train_dataset, test_dataset, class_names):\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')} - Starting training...\")\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=len(class_names)\n",
        "    ).to(device)\n",
        "\n",
        "    # Optimized training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=2,  # Reduced from 3\n",
        "        per_device_train_batch_size=8,  # Reduced from 16\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=4,  # Simulates larger batch size\n",
        "        warmup_steps=100,  # Reduced from 500\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        # eval_strategy=\"steps\", # Temporarily removed\n",
        "        # eval_steps=500,  # Temporarily removed\n",
        "        # save_strategy=\"steps\", # Temporarily removed\n",
        "        # save_steps=500, # Temporarily removed\n",
        "        # load_best_model_at_end=True, # Temporarily removed\n",
        "        report_to=\"none\",\n",
        "        fp16=torch.cuda.is_available(),  # Enable mixed precision\n",
        "        dataloader_num_workers=4 if torch.cuda.is_available() else 2,\n",
        "        gradient_checkpointing=False  # Disabled gradient checkpointing\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(f\"{datetime.now().strftime('%H:%M:%S')} - Training complete\")\n",
        "    return model\n",
        "\n",
        "def create_gradio_interface(model, tokenizer, class_names):\n",
        "    def classify_headline(text):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=96\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        top_prob, top_label = torch.max(probs, dim=1)\n",
        "\n",
        "        return {\n",
        "            \"Predicted Category\": class_names[top_label.item()],\n",
        "            \"Confidence\": f\"{top_prob.item():.4f}\"\n",
        "        }\n",
        "\n",
        "    return gr.Interface(\n",
        "        fn=classify_headline,\n",
        "        inputs=gr.Textbox(label=\"News Headline\"),\n",
        "        outputs=gr.Label(label=\"Classification Result\"),\n",
        "        title=\"News Topic Classifier\",\n",
        "        examples=[\n",
        "            [\"Apple announces new iPhone with revolutionary features\"],\n",
        "            [\"Football team wins championship after dramatic final match\"]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    train_texts, train_labels, test_texts, test_labels, class_names = load_and_prepare_data()\n",
        "\n",
        "    # Tokenize\n",
        "    train_dataset, test_dataset, tokenizer = tokenize_data(\n",
        "        train_texts, train_labels, test_texts, test_labels\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    model = train_model(train_dataset, test_dataset, class_names)\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = evaluate_model(model, test_dataset)\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "    # Deploy\n",
        "    print(\"\\nLaunching Gradio interface...\")\n",
        "    interface = create_gradio_interface(model, tokenizer, class_names)\n",
        "    interface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FOilznqh4mh1",
        "outputId": "9939d2fd-71df-40d6-e44f-12d7b07daaeb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "11:48:29 - Loading dataset...\n",
            "11:48:31 - Dataset loaded\n",
            "11:48:31 - Tokenizing data...\n",
            "11:48:39 - Tokenization complete\n",
            "11:48:39 - Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 02:09, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.177500</td>\n",
              "      <td>0.227460</td>\n",
              "      <td>0.928000</td>\n",
              "      <td>0.927906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11:51:31 - Training complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1836480658.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
            "/tmp/ipython-input-6-1836480658.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfaeezsalman939\u001b[0m (\u001b[33mfaeezsalman939-bahria-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250722_115458-9v9o3vyt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/faeezsalman939-bahria-university/huggingface/runs/9v9o3vyt' target=\"_blank\">tmp_trainer</a></strong> to <a href='https://wandb.ai/faeezsalman939-bahria-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/faeezsalman939-bahria-university/huggingface' target=\"_blank\">https://wandb.ai/faeezsalman939-bahria-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/faeezsalman939-bahria-university/huggingface/runs/9v9o3vyt' target=\"_blank\">https://wandb.ai/faeezsalman939-bahria-university/huggingface/runs/9v9o3vyt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9280\n",
            "F1 Score: 0.9279\n",
            "\n",
            "Launching Gradio interface...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ae7e887c5c07b789f8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae7e887c5c07b789f8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh92OIlH79t3",
        "outputId": "015563d8-f282-43d7-c13b-3532cd667de4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    }
  ]
}